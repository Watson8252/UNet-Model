# -*- coding: utf-8 -*-
"""Unet_Model_Train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SFJn43-yiW0RASmkNvG6Sqmmb7mfMMfq

# Train U-Net Binary Model

Author: Erwin Cazares, Maria Reyna and Alexander Watson <br/>
Projeect: MNIST: HAM10000 dataset U-Net Segmentation
"""

import logging
import warnings
import os
logging.getLogger("tensorflow").setLevel(logging.ERROR)
warnings.filterwarnings('ignore')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# Import Libraries
#from tensorflow.keras.utils import normalize
import time
import os
import cv2
import pandas as pd
from PIL import Image, ImageFilter
import numpy as np
from matplotlib import pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import glob
import tensorflow as tf
import pydot
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization
from tensorflow.keras.layers import Dense, Lambda, Reshape, Activation, MaxPool2D, Concatenate, Dropout, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.utils import plot_model
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import Sequence

"""Look for GPU devices, the list must be at least 1"""

physical_devices = tf.config.list_physical_devices('GPU')
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
tf.config.experimental.set_memory_growth(physical_devices[0], True)
tf.compat.v1.enable_eager_execution()

folder_path = 'D:\CS6361_Project'

# List contents of the root directory in your Google Drive
!ls $folder_path

print("Current working dir : %s" % os.getcwd())

# Define print statements for training

def print_best_metrics(h):
    labels = ['training accuracy', 'validation accuracy', 'training loss', 'validation loss']
    keys = ['accuracy', 'val_accuracy', 'loss', 'val_loss']
    prefix = ['highest','lowest']
    for i in range(4):
        metric = np.array(h[keys[i]])
        am = np.argmax((1.5-i)*metric)
        print(f'The {prefix[i//2]} {labels[i]} was {metric[am]:.4f} obtained in epoch {am+1}')
        print(f'The final {labels[i]} was {metric[-1]:.4f}')
        print()

# Define the IoU loss function
def iou_loss(y_true, y_pred):
    # Cast both y_true and y_pred to float32
    y_true = tf.dtypes.cast(y_true, tf.float32)
    y_pred = tf.dtypes.cast(y_pred, tf.float32)

    intersection = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection
    iou = (intersection + 1e-6) / (union + 1e-6)
    return 1 - iou

# Define the hybrid loss function
def hybrid_loss(y_true, y_pred):
    # Cast y_pred to float32 (you can also cast y_true for consistency)
    y_pred = tf.dtypes.cast(y_pred, tf.float32)

    bce_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)
    iou = iou_loss(y_true, y_pred)
    return bce_loss + iou



# Define metrics to use
precision_metric = tf.keras.metrics.Precision()
recall_metric = tf.keras.metrics.Recall()
metrics = ['accuracy', precision_metric, recall_metric]

#Define image directories
dataset_directory = os.path.join(folder_path, 'Dataset')
images_folder_path = os.path.join(dataset_directory, 'images')
masks_folder_path = os.path.join(dataset_directory, 'masks')
results_folder = os.path.join(folder_path, 'results/')
weights_folder = os.path.join(folder_path, 'weights/')

# Define notebook variables for training the models
n_classes = 1
img_aug = False
width = 128
height = 96

# Custom generator class
class CustomDataGenerator(Sequence):
    def __init__(self, dataframe, image_path_col, mask_path_col, batch_size, target_size):
        self.dataframe = dataframe
        self.image_path_col = image_path_col
        self.mask_path_col = mask_path_col
        self.batch_size = batch_size
        self.target_size = target_size
        self.indexes = np.arange(len(self.dataframe))

    def __len__(self):
        return int(np.ceil(len(self.dataframe) / self.batch_size))

    def __getitem__(self, index):
        start = index * self.batch_size
        end = (index + 1) * self.batch_size
        batch_x = self.dataframe.iloc[start:end, :][self.image_path_col].apply(lambda x: cv2.imread(x, -1)[..., ::-1])
        batch_y = self.dataframe.iloc[start:end, :][self.mask_path_col].apply(lambda x: cv2.imread(x, cv2.IMREAD_GRAYSCALE))

        batch_x = np.stack(batch_x.values)
        batch_y = np.expand_dims(np.stack(batch_y.values), axis=-1)

        # Reshape images and masks to the desired target size
        batch_x = np.array([cv2.resize(x, self.target_size) for x in batch_x])
        batch_y = np.array([cv2.resize(y, self.target_size) for y in batch_y])

        batch_x = batch_x / 255.0
        batch_y = batch_y / 255.0

        batch_y = np.uint8(np.expand_dims(batch_y, axis=-1))

        return batch_x, batch_y

def visualize_images_masks(generator, num_samples=3):
    # Get a batch of images and masks from the generator
    images, masks = generator.__getitem__(0)

    # Display the first `num_samples` images and masks
    for i in range(num_samples):
        plt.figure(figsize=(12, 6))

        # Display the image
        plt.subplot(1, 2, 1)
        plt.imshow(images[i])
        plt.title('Image')
        plt.axis('off')

        # Display the mask
        plt.subplot(1, 2, 2)
        plt.imshow(masks[i, :, :, 0], cmap='gray')
        plt.title('Mask')
        plt.axis('off')

        plt.show()

# Get the list of filenames in images and masks folders
image_filenames = [os.path.join(images_folder_path, img) for img in os.listdir(images_folder_path)]
mask_filenames = [os.path.join(masks_folder_path, mask) for mask in os.listdir(masks_folder_path)]

image_filenames.sort()
mask_filenames.sort()

# Merge DataFrames based on the filename column
names_df = pd.DataFrame({'image':image_filenames, 'masks':mask_filenames})
names_df.head()

# Split the data into training and validation sets
train_df, test_df = train_test_split(names_df, test_size=0.1, random_state=42)
train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)

# Create generators for training and validation
train_generator = CustomDataGenerator(train_df, 'image', 'masks', batch_size=5, target_size=(width, height))
val_generator = CustomDataGenerator(val_df, 'image', 'masks', batch_size=5, target_size=(width, height))
test_generator = CustomDataGenerator(test_df, 'image', 'masks', batch_size=5, target_size=(width, height))

for data_batch, labels_batch in train_generator:
    print("Image batch shape:", data_batch.shape)
    print(f'Image dtype: {data_batch[0].dtype}')
    print(f'Image Min Pixel Value: {np.min(data_batch)}')
    print(f'Image Max Pixel Value: {np.max(data_batch)}')
    print()
    print("Mask batch shape:", labels_batch.shape)
    print(f'Mask dtype: {labels_batch[0].dtype}')
    print(f'Mask labels: {np.unique(labels_batch)}')
    print()
    print(len(train_generator))
    break

# Visualize images and masks using the training generator
visualize_images_masks(train_generator, num_samples=3)

# Define input for model
input_shape = (data_batch.shape[1], data_batch.shape[2],data_batch.shape[3])
print(f'Input shape for model = {input_shape}')

"""## Original U-net model, proposed by Ronnerberg et al."""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate

def conv_block(x, num_filters, kernel_size=3):
    x = Conv2D(num_filters, 3, padding="same", activation="relu")(x)
    x = Conv2D(num_filters, 3, padding="same", activation="relu")(x)
    return x

def encoder_block(input_tensor, num_filters):
    conv = conv_block(input_tensor, num_filters)
    pool = MaxPooling2D((2, 2))(conv)
    return conv, pool

def decoder_block(input_tensor, skip_tensor, num_filters):
    upsampled = UpSampling2D((2, 2))(input_tensor)
    concatenated = concatenate([skip_tensor, upsampled], axis=-1)
    x = conv_block(concatenated, num_filters)
    return x

def unet_original(input_shape=(256, 256, 1)):
    inputs = Input(input_shape)

    # Contracting Path
    conv1, pool1 = encoder_block(inputs, 64)
    conv2, pool2 = encoder_block(pool1, 128)
    conv3, pool3 = encoder_block(pool2, 256)
    conv4, pool4 = encoder_block(pool3, 512)

    # Bottom
    conv5 = conv_block(pool4, 1024)

    # Expansive Path
    conv6 = decoder_block(conv5, conv4, 512)
    conv7 = decoder_block(conv6, conv3, 256)
    conv8 = decoder_block(conv7, conv2, 128)
    conv9 = decoder_block(conv8, conv1, 64)

    # Output layer
    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)

    model = Model(inputs=inputs, outputs=outputs, name = 'Original_UNet')

    return model

# Create the U-Net model
model = unet_original((96,128,3))
model.summary()

from tensorflow.keras.utils import plot_model

# Plot the model
plot_model(model, to_file='unet_original.png', show_shapes=True, show_layer_names=True)

# Train different Unet models with different learning rates
model_data = {}
lr = 0.0001

# Load optimizer
optim = tf.keras.optimizers.Adam(learning_rate = lr)
model.compile(optim, hybrid_loss, metrics=metrics)

print(f'Training {model.name}, using {lr} for the learning rate')
print()

start_train = time.time()

history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator) // 7, # 1030 training samples per epoch
    epochs=45,
    verbose=1,
    validation_data=val_generator,
    validation_steps=len(val_generator) // 10 # 180 val samples per epoch
)

model.save(weights_folder + model.name + ".hdf5")

end_train = time.time()


print()
history.history["training_time"] = end_train - start_train
print(f'Elapsed time training (learning rate {lr}) = {end_train - start_train:.4f} secs')
print()

print_best_metrics(history.history)

model_data[model.name] = history.history

# Clear the TensorFlow session and release GPU memory
tf.keras.backend.clear_session()

# Path to the JSON file
import json
json_file_path = results_folder + model.name + '_data.json'

# Save the data to a JSON file
with open(json_file_path, 'w') as json_file:
    json.dump(model_data, json_file, indent=4)

print(f'Data saved to {json_file_path}')

#plot the training and validation accuracy and loss at each epoch
# Extract accuracy and loss data
epochs = range(1, len(history.history['accuracy']) + 1)

# Create subplots with 1 row and 2 columns
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Plot accuracy
axes[0, 0].plot(epochs, history.history['accuracy'], 'y', label='Training acc')
axes[0, 0].plot(epochs, history.history['val_accuracy'], 'r', label='Validation acc')
axes[0, 0].set_title('Training and Validation Accuracy')
axes[0, 0].set_xlabel('Epochs')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].legend()

# Plot loss
axes[0, 1].plot(epochs, history.history['loss'], 'y', label='Training loss')
axes[0, 1].plot(epochs, history.history['val_loss'], 'r', label='Validation loss')
axes[0, 1].set_title('Training and Validation loss')
axes[0, 1].set_xlabel('Epochs')
axes[0, 1].set_ylabel('Loss')
axes[0, 1].legend()

# Plot Precision
axes[1, 0].plot(epochs, history.history['precision'], 'y', label='Training Precision')
axes[1, 0].plot(epochs, history.history['val_precision'], 'r', label='Validation Precision')
axes[1, 0].set_title('Training and Validation Precision')
axes[1, 0].set_xlabel('Epochs')
axes[1, 0].set_ylabel('Loss')
axes[1, 0].legend()

# Plot Recall
axes[1, 1].plot(epochs, history.history['recall'], 'y', label='Training Recall')
axes[1, 1].plot(epochs, history.history['val_recall'], 'r', label='Validation Recall')
axes[1, 1].set_title('Training and Validation Recall')
axes[1, 1].set_xlabel('Epochs')
axes[1, 1].set_ylabel('Loss')
axes[1, 1].legend()

# Global title
fig.suptitle(f'Training and Validation Metrics for {model.name}', fontsize=15)

# Adjust layout
plt.tight_layout()

# Save the plots
plt.savefig(results_folder + f'{model.name}_metrics.png')

plt.show()

# Get a batch of images and ground truth masks from the validation generator

# Choose a few random indices from the batch
num_samples = 5

# Plot the images, ground truth masks, and predicted masks
for i in range(num_samples):
    plt.figure(figsize=(15, 5))

    # Randomly select an index from the length of the generator
    random_index = np.random.choice(len(val_generator), replace=False)

    # Fetch a batch of data from the generator
    val_batch_x, val_batch_y = val_generator[random_index]
    # Predict on the validation batch
    predictions = model.predict(val_batch_x)
    # Original Image
    plt.subplot(1, 3, 1)
    plt.imshow(val_batch_x[0,:,:,:])
    plt.title('Original Image')
    plt.axis('off')

    # Ground Truth Mask
    plt.subplot(1, 3, 2)
    plt.imshow(val_batch_y[0, :, :, 0], cmap='gray')
    plt.title('Ground Truth Mask')
    plt.axis('off')

    # Predicted Mask
    plt.subplot(1, 3, 3)
    plt.imshow(predictions[0, :, :, 0] > 0.50, cmap='gray')
    plt.title('Predicted Mask')
    plt.axis('off')
    plt.savefig(results_folder + f'{model.name}_pred{i}.png')
    plt.show()

"""## Modified U-net 1:

- Reduced number of filters at each layer by half.
"""

def unet_1(input_shape=(256, 256, 1)):
    inputs = Input(input_shape)

    # Contracting Path
    conv1, pool1 = encoder_block(inputs, 32)
    conv2, pool2 = encoder_block(pool1, 64)
    conv3, pool3 = encoder_block(pool2, 128)
    conv4, pool4 = encoder_block(pool3, 256)

    # Bottom
    conv5 = conv_block(pool4, 512)

    # Expansive Path
    conv6 = decoder_block(conv5, conv4, 256)
    conv7 = decoder_block(conv6, conv3, 128)
    conv8 = decoder_block(conv7, conv2, 64)
    conv9 = decoder_block(conv8, conv1, 32)

    # Output layer
    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)

    model = Model(inputs=inputs, outputs=outputs, name = 'Modified_UNet_01')

    return model

# Create the U-Net model
model = unet_1(input_shape)
model.summary()

# Train different Unet models with different learning rates
model_data = {}
lr = 0.0001

# Load optimizer
optim = tf.keras.optimizers.Adam(learning_rate = lr)
model.compile(optim, hybrid_loss, metrics=metrics)

print(f'Training {model.name}, using {lr} for the learning rate')
print()

start_train = time.time()

history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator) // 7,
    epochs=45,
    verbose=1,
    validation_data=val_generator,
    validation_steps=len(val_generator) // 10
)

model.save(weights_folder + model.name + ".hdf5")

end_train = time.time()


print()
history.history["training_time"] = end_train - start_train
print(f'Elapsed time training (learning rate {lr}) = {end_train - start_train:.4f} secs')
print()

print_best_metrics(history.history)

model_data[model.name] = history.history

# Clear the TensorFlow session and release GPU memory
tf.keras.backend.clear_session()

# Path to the JSON file
import json
json_file_path = results_folder + model.name + '_data.json'

# Save the data to a JSON file
with open(json_file_path, 'w') as json_file:
    json.dump(model_data, json_file, indent=4)

print(f'Data saved to {json_file_path}')

#plot the training and validation accuracy and loss at each epoch
# Extract accuracy and loss data
epochs = range(1, len(history.history['accuracy']) + 1)

# Create subplots with 1 row and 2 columns
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Plot accuracy
axes[0, 0].plot(epochs, history.history['accuracy'], 'y', label='Training acc')
axes[0, 0].plot(epochs, history.history['val_accuracy'], 'r', label='Validation acc')
axes[0, 0].set_title('Training and Validation Accuracy')
axes[0, 0].set_xlabel('Epochs')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].legend()

# Plot loss
axes[0, 1].plot(epochs, history.history['loss'], 'y', label='Training loss')
axes[0, 1].plot(epochs, history.history['val_loss'], 'r', label='Validation loss')
axes[0, 1].set_title('Training and Validation loss')
axes[0, 1].set_xlabel('Epochs')
axes[0, 1].set_ylabel('Loss')
axes[0, 1].legend()

# Plot Precision
axes[1, 0].plot(epochs, history.history['precision'], 'y', label='Training Precision')
axes[1, 0].plot(epochs, history.history['val_precision'], 'r', label='Validation Precision')
axes[1, 0].set_title('Training and Validation Precision')
axes[1, 0].set_xlabel('Epochs')
axes[1, 0].set_ylabel('Loss')
axes[1, 0].legend()

# Plot Recall
axes[1, 1].plot(epochs, history.history['recall'], 'y', label='Training Recall')
axes[1, 1].plot(epochs, history.history['val_recall'], 'r', label='Validation Recall')
axes[1, 1].set_title('Training and Validation Recall')
axes[1, 1].set_xlabel('Epochs')
axes[1, 1].set_ylabel('Loss')
axes[1, 1].legend()

# Global title
fig.suptitle(f'Training and Validation Metrics for {model.name}', fontsize=15)

# Adjust layout
plt.tight_layout()

# Save the plots
plt.savefig(results_folder + f'{model.name}_metrics.png')

# Get a batch of images and ground truth masks from the validation generator


# Choose a few random indices from the batch
num_samples = 5

# Plot the images, ground truth masks, and predicted masks
for i in range(num_samples):
    plt.figure(figsize=(15, 5))

    # Randomly select an index from the length of the generator
    random_index = np.random.choice(len(val_generator), replace=False)

    # Fetch a batch of data from the generator
    val_batch_x, val_batch_y = val_generator[random_index]
    # Predict on the validation batch
    predictions = model.predict(val_batch_x)
    # Original Image
    plt.subplot(1, 3, 1)
    plt.imshow(val_batch_x[0,:,:,:])
    plt.title('Original Image')
    plt.axis('off')

    # Ground Truth Mask
    plt.subplot(1, 3, 2)
    plt.imshow(val_batch_y[0, :, :, 0], cmap='gray')
    plt.title('Ground Truth Mask')
    plt.axis('off')

    # Predicted Mask
    plt.subplot(1, 3, 3)
    plt.imshow(predictions[0, :, :, 0] > 0.50, cmap='gray')
    plt.title('Predicted Mask')
    plt.axis('off')
    plt.savefig(results_folder + f'{model.name}_pred{i}.png')
    plt.show()

"""## Modified U-net 2:

- Reduced the number of convulutional blocks from 5 to 4.
"""

def unet_2(input_shape=(256, 256, 1)):
    inputs = Input(input_shape)

    # Contracting Path
    conv1, pool1 = encoder_block(inputs, 64)
    conv2, pool2 = encoder_block(pool1, 128)
    conv3, pool3 = encoder_block(pool2, 256)

    # Bottom
    conv4 = conv_block(pool3, 512)

    # Expansive Path
    conv5 = decoder_block(conv4, conv3, 512)
    conv6 = decoder_block(conv5, conv2, 256)
    conv7 = decoder_block(conv6, conv1, 128)

    # Output layer
    outputs = Conv2D(1, 1, activation='sigmoid')(conv7)

    model = Model(inputs=inputs, outputs=outputs, name = 'Modified_UNet_02')

    return model

# Create the U-Net model
model = unet_2(input_shape)
model.summary()

# Train different Unet models with different learning rates
model_data = {}
lr = 0.0001

# Load optimizer
optim = tf.keras.optimizers.Adam(learning_rate = lr)
model.compile(optim, hybrid_loss, metrics=metrics)

print(f'Training {model.name}, using {lr} for the learning rate')
print()

start_train = time.time()

history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator) // 7,
    epochs=45,
    verbose=1,
    validation_data=val_generator,
    validation_steps=len(val_generator) // 10
)

model.save(weights_folder + model.name + ".hdf5")

end_train = time.time()


print()
history.history["training_time"] = end_train - start_train
print(f'Elapsed time training (learning rate {lr}) = {end_train - start_train:.4f} secs')
print()

print_best_metrics(history.history)

model_data[model.name] = history.history

# Clear the TensorFlow session and release GPU memory
tf.keras.backend.clear_session()

# Path to the JSON file
import json
json_file_path = results_folder + model.name + '_data.json'

# Save the data to a JSON file
with open(json_file_path, 'w') as json_file:
    json.dump(model_data, json_file, indent=4)

print(f'Data saved to {json_file_path}')

#plot the training and validation accuracy and loss at each epoch
# Extract accuracy and loss data
epochs = range(1, len(history.history['accuracy']) + 1)

# Create subplots with 1 row and 2 columns
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Plot accuracy
axes[0, 0].plot(epochs, history.history['accuracy'], 'y', label='Training acc')
axes[0, 0].plot(epochs, history.history['val_accuracy'], 'r', label='Validation acc')
axes[0, 0].set_title('Training and Validation Accuracy')
axes[0, 0].set_xlabel('Epochs')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].legend()

# Plot loss
axes[0, 1].plot(epochs, history.history['loss'], 'y', label='Training loss')
axes[0, 1].plot(epochs, history.history['val_loss'], 'r', label='Validation loss')
axes[0, 1].set_title('Training and Validation loss')
axes[0, 1].set_xlabel('Epochs')
axes[0, 1].set_ylabel('Loss')
axes[0, 1].legend()

# Plot Precision
axes[1, 0].plot(epochs, history.history['precision'], 'y', label='Training Precision')
axes[1, 0].plot(epochs, history.history['val_precision'], 'r', label='Validation Precision')
axes[1, 0].set_title('Training and Validation Precision')
axes[1, 0].set_xlabel('Epochs')
axes[1, 0].set_ylabel('Loss')
axes[1, 0].legend()

# Plot Recall
axes[1, 1].plot(epochs, history.history['recall'], 'y', label='Training Recall')
axes[1, 1].plot(epochs, history.history['val_recall'], 'r', label='Validation Recall')
axes[1, 1].set_title('Training and Validation Recall')
axes[1, 1].set_xlabel('Epochs')
axes[1, 1].set_ylabel('Loss')
axes[1, 1].legend()

# Global title
fig.suptitle(f'Training and Validation Metrics for {model.name}', fontsize=15)

# Adjust layout
plt.tight_layout()

# Save the plots
plt.savefig(results_folder + f'{model.name}_metrics.png')

plt.show()

# Get a batch of images and ground truth masks from the validation generator


# Choose a few random indices from the batch
num_samples = 5

# Plot the images, ground truth masks, and predicted masks
for i in range(num_samples):
    plt.figure(figsize=(15, 5))

    # Randomly select an index from the length of the generator
    random_index = np.random.choice(len(val_generator), replace=False)

    # Fetch a batch of data from the generator
    val_batch_x, val_batch_y = val_generator[random_index]
    # Predict on the validation batch
    predictions = model.predict(val_batch_x)
    # Original Image
    plt.subplot(1, 3, 1)
    plt.imshow(val_batch_x[0,:,:,:])
    plt.title('Original Image')
    plt.axis('off')

    # Ground Truth Mask
    plt.subplot(1, 3, 2)
    plt.imshow(val_batch_y[0, :, :, 0], cmap='gray')
    plt.title('Ground Truth Mask')
    plt.axis('off')

    # Predicted Mask
    plt.subplot(1, 3, 3)
    plt.imshow(predictions[0, :, :, 0] > 0.50, cmap='gray')
    plt.title('Predicted Mask')
    plt.axis('off')
    plt.savefig(results_folder + f'{model.name}_pred{i}.png')
    plt.show()

"""## Modified U-net 3:

- Reduced the number of convulutional blocks from 5 to 4.
- Reduced the number of filters at each convulutional layer by half.
"""

def unet_3(input_shape=(256, 256, 1)):
    inputs = Input(input_shape)

    # Contracting Path
    conv1, pool1 = encoder_block(inputs, 32)
    conv2, pool2 = encoder_block(pool1, 64)
    conv3, pool3 = encoder_block(pool2, 128)

    # Bottom
    conv4 = conv_block(pool3, 256)

    # Expansive Path
    conv5 = decoder_block(conv4, conv3, 128)
    conv6 = decoder_block(conv5, conv2, 64)
    conv7 = decoder_block(conv6, conv1, 32)

    # Output layer
    outputs = Conv2D(1, 1, activation='sigmoid')(conv7)

    model = Model(inputs=inputs, outputs=outputs, name = 'Modified_UNet_03')

    return model

# Create the U-Net model
model = unet_3(input_shape)
model.summary()

# Train different Unet models with different learning rates
model_data = {}
lr = 0.0001

# Load optimizer
optim = tf.keras.optimizers.Adam(learning_rate = lr)
model.compile(optim, hybrid_loss, metrics=metrics)

print(f'Training {model.name}, using {lr} for the learning rate')
print()

start_train = time.time()

history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator) // 7,
    epochs=45,
    verbose=1,
    validation_data=val_generator,
    validation_steps=len(val_generator) // 10
)

model.save(weights_folder + model.name + ".hdf5")

end_train = time.time()


print()
history.history["training_time"] = end_train - start_train
print(f'Elapsed time training (learning rate {lr}) = {end_train - start_train:.4f} secs')
print()

print_best_metrics(history.history)

model_data[model.name] = history.history

# Clear the TensorFlow session and release GPU memory
tf.keras.backend.clear_session()

# Path to the JSON file
import json
json_file_path = results_folder + model.name + '_data.json'

# Save the data to a JSON file
with open(json_file_path, 'w') as json_file:
    json.dump(model_data, json_file, indent=4)

print(f'Data saved to {json_file_path}')

#plot the training and validation accuracy and loss at each epoch
# Extract accuracy and loss data
epochs = range(1, len(history.history['accuracy']) + 1)

# Create subplots with 1 row and 2 columns
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Plot accuracy
axes[0, 0].plot(epochs, history.history['accuracy'], 'y', label='Training acc')
axes[0, 0].plot(epochs, history.history['val_accuracy'], 'r', label='Validation acc')
axes[0, 0].set_title('Training and Validation Accuracy')
axes[0, 0].set_xlabel('Epochs')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].legend()

# Plot loss
axes[0, 1].plot(epochs, history.history['loss'], 'y', label='Training loss')
axes[0, 1].plot(epochs, history.history['val_loss'], 'r', label='Validation loss')
axes[0, 1].set_title('Training and Validation loss')
axes[0, 1].set_xlabel('Epochs')
axes[0, 1].set_ylabel('Loss')
axes[0, 1].legend()

# Plot Precision
axes[1, 0].plot(epochs, history.history['precision'], 'y', label='Training Precision')
axes[1, 0].plot(epochs, history.history['val_precision'], 'r', label='Validation Precision')
axes[1, 0].set_title('Training and Validation Precision')
axes[1, 0].set_xlabel('Epochs')
axes[1, 0].set_ylabel('Loss')
axes[1, 0].legend()

# Plot Recall
axes[1, 1].plot(epochs, history.history['recall'], 'y', label='Training Recall')
axes[1, 1].plot(epochs, history.history['val_recall'], 'r', label='Validation Recall')
axes[1, 1].set_title('Training and Validation Recall')
axes[1, 1].set_xlabel('Epochs')
axes[1, 1].set_ylabel('Loss')
axes[1, 1].legend()

# Global title
fig.suptitle(f'Training and Validation Metrics for {model.name}', fontsize=15)

# Adjust layout
plt.tight_layout()

# Save the plots
plt.savefig(results_folder + f'{model.name}_metrics.png')

plt.show()

# Get a batch of images and ground truth masks from the validation generator


# Choose a few random indices from the batch
num_samples = 5

# Plot the images, ground truth masks, and predicted masks
for i in range(num_samples):
    plt.figure(figsize=(15, 5))

    # Randomly select an index from the length of the generator
    random_index = np.random.choice(len(val_generator), replace=False)

    # Fetch a batch of data from the generator
    val_batch_x, val_batch_y = val_generator[random_index]
    # Predict on the validation batch
    predictions = model.predict(val_batch_x)
    # Original Image
    plt.subplot(1, 3, 1)
    plt.imshow(val_batch_x[0,:,:,:])
    plt.title('Original Image')
    plt.axis('off')

    # Ground Truth Mask
    plt.subplot(1, 3, 2)
    plt.imshow(val_batch_y[0, :, :, 0], cmap='gray')
    plt.title('Ground Truth Mask')
    plt.axis('off')

    # Predicted Mask
    plt.subplot(1, 3, 3)
    plt.imshow(predictions[0, :, :, 0] > 0.50, cmap='gray')
    plt.title('Predicted Mask')
    plt.axis('off')
    plt.savefig(results_folder + f'{model.name}_pred{i}.png')
    plt.show()

"""## Modified U-net 4:

- Added dropout layers after the convulutional layers using the orginal network.
"""

# Define the new model

def conv_block(x, num_filters, kernel_size=3, dropout_rate=0.5):
    x = Conv2D(num_filters, 3, padding="same", activation="relu")(x)
    x = Conv2D(num_filters, 3, padding="same", activation="relu")(x)
    x = Dropout(dropout_rate)(x) # Added for this modified network
    return x

def unet_4(input_shape=(256, 256, 1)):
    inputs = Input(input_shape)

    # Contracting Path
    conv1, pool1 = encoder_block(inputs, 64)
    conv2, pool2 = encoder_block(pool1, 128)
    conv3, pool3 = encoder_block(pool2, 256)
    conv4, drop4 = encoder_block(pool3, 512)

    # Bottom
    conv5 = conv_block(drop4, 1024)

    # Expansive Path
    conv6 = decoder_block(conv5, conv4, 512)
    conv7 = decoder_block(conv6, conv3, 256)
    conv8 = decoder_block(conv7, conv2, 128)
    conv9 = decoder_block(conv8, conv1, 64)

    # Output layer
    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)

    model = Model(inputs=inputs, outputs=outputs, name = 'Modified_UNet_04')

    return model

# Create the U-Net model
model = unet_4(input_shape)
model.summary()

# Train different Unet models with different learning rates
model_data = {}
lr = 0.0001

# Load optimizer
optim = tf.keras.optimizers.Adam(learning_rate = lr)
model.compile(optim, hybrid_loss, metrics=metrics)

print(f'Training {model.name}, using {lr} for the learning rate')
print()

start_train = time.time()

history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator) // 7,
    epochs=45,
    verbose=1,
    validation_data=val_generator,
    validation_steps=len(val_generator) // 10
)

model.save(weights_folder + model.name + ".hdf5")

end_train = time.time()


print()
history.history["training_time"] = end_train - start_train
print(f'Elapsed time training (learning rate {lr}) = {end_train - start_train:.4f} secs')
print()

print_best_metrics(history.history)

model_data[model.name] = history.history

# Clear the TensorFlow session and release GPU memory
tf.keras.backend.clear_session()

# Path to the JSON file
import json
json_file_path = results_folder + model.name + '_data.json'

# Save the data to a JSON file
with open(json_file_path, 'w') as json_file:
    json.dump(model_data, json_file, indent=4)

print(f'Data saved to {json_file_path}')

#plot the training and validation accuracy and loss at each epoch
# Extract accuracy and loss data
epochs = range(1, len(history.history['accuracy']) + 1)

# Create subplots with 1 row and 2 columns
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Plot accuracy
axes[0, 0].plot(epochs, history.history['accuracy'], 'y', label='Training acc')
axes[0, 0].plot(epochs, history.history['val_accuracy'], 'r', label='Validation acc')
axes[0, 0].set_title('Training and Validation Accuracy')
axes[0, 0].set_xlabel('Epochs')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].legend()

# Plot loss
axes[0, 1].plot(epochs, history.history['loss'], 'y', label='Training loss')
axes[0, 1].plot(epochs, history.history['val_loss'], 'r', label='Validation loss')
axes[0, 1].set_title('Training and Validation loss')
axes[0, 1].set_xlabel('Epochs')
axes[0, 1].set_ylabel('Loss')
axes[0, 1].legend()

# Plot Precision
axes[1, 0].plot(epochs, history.history['precision'], 'y', label='Training Precision')
axes[1, 0].plot(epochs, history.history['val_precision'], 'r', label='Validation Precision')
axes[1, 0].set_title('Training and Validation Precision')
axes[1, 0].set_xlabel('Epochs')
axes[1, 0].set_ylabel('Loss')
axes[1, 0].legend()

# Plot Recall
axes[1, 1].plot(epochs, history.history['recall'], 'y', label='Training Recall')
axes[1, 1].plot(epochs, history.history['val_recall'], 'r', label='Validation Recall')
axes[1, 1].set_title('Training and Validation Recall')
axes[1, 1].set_xlabel('Epochs')
axes[1, 1].set_ylabel('Loss')
axes[1, 1].legend()

# Global title
fig.suptitle(f'Training and Validation Metrics for {model.name}', fontsize=15)

# Adjust layout
plt.tight_layout()

# Save the plots
plt.savefig(results_folder + f'{model.name}_metrics.png')

plt.show()

# Get a batch of images and ground truth masks from the validation generator


# Choose a few random indices from the batch
num_samples = 5

# Plot the images, ground truth masks, and predicted masks
for i in range(num_samples):
    plt.figure(figsize=(15, 5))

    # Randomly select an index from the length of the generator
    random_index = np.random.choice(len(val_generator), replace=False)

    # Fetch a batch of data from the generator
    val_batch_x, val_batch_y = val_generator[random_index]
    # Predict on the validation batch
    predictions = model.predict(val_batch_x)
    # Original Image
    plt.subplot(1, 3, 1)
    plt.imshow(val_batch_x[0,:,:,:])
    plt.title('Original Image')
    plt.axis('off')

    # Ground Truth Mask
    plt.subplot(1, 3, 2)
    plt.imshow(val_batch_y[0, :, :, 0], cmap='gray')
    plt.title('Ground Truth Mask')
    plt.axis('off')

    # Predicted Mask
    plt.subplot(1, 3, 3)
    plt.imshow(predictions[0, :, :, 0] > 0.50, cmap='gray')
    plt.title('Predicted Mask')
    plt.axis('off')
    plt.savefig(results_folder + f'{model.name}_pred{i}.png')
    plt.show()

"""## Modified U-net 5:

- Added dropout layers after the convulutional layers using the orginal network.
- Reduced the number of convulutional blocks from 5 to 4.

"""

def unet_5(input_shape=(256, 256, 1)):
    inputs = Input(input_shape)

    # Contracting Path
    conv1, pool1 = encoder_block(inputs, 64)
    conv2, pool2 = encoder_block(pool1, 128)
    conv3, pool3 = encoder_block(pool2, 256)

    # Bottom
    conv4 = conv_block(pool3, 512)

    # Expansive Path
    conv5 = decoder_block(conv4, conv3, 512)
    conv6 = decoder_block(conv5, conv2, 256)
    conv7 = decoder_block(conv6, conv1, 128)

    # Output layer
    outputs = Conv2D(1, 1, activation='sigmoid')(conv7)

    model = Model(inputs=inputs, outputs=outputs, name = 'Modified_UNet_05')

    return model

# Create the U-Net model
model = unet_5(input_shape)
model.summary()

# Train different Unet models with different learning rates
model_data = {}
lr = 0.0001

# Load optimizer
optim = tf.keras.optimizers.Adam(learning_rate = lr)
model.compile(optim, hybrid_loss, metrics=metrics)

print(f'Training {model.name}, using {lr} for the learning rate')
print()

start_train = time.time()

history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator) // 7,
    epochs=45,
    verbose=1,
    validation_data=val_generator,
    validation_steps=len(val_generator) // 10
)

model.save(weights_folder + model.name + ".hdf5")

end_train = time.time()


print()
history.history["training_time"] = end_train - start_train
print(f'Elapsed time training (learning rate {lr}) = {end_train - start_train:.4f} secs')
print()

print_best_metrics(history.history)

model_data[model.name] = history.history

# Clear the TensorFlow session and release GPU memory
tf.keras.backend.clear_session()

# Path to the JSON file
import json
json_file_path = results_folder + model.name + '_data.json'

# Save the data to a JSON file
with open(json_file_path, 'w') as json_file:
    json.dump(model_data, json_file, indent=4)

print(f'Data saved to {json_file_path}')

#plot the training and validation accuracy and loss at each epoch
# Extract accuracy and loss data
epochs = range(1, len(history.history['accuracy']) + 1)

# Create subplots with 1 row and 2 columns
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Plot accuracy
axes[0, 0].plot(epochs, history.history['accuracy'], 'y', label='Training acc')
axes[0, 0].plot(epochs, history.history['val_accuracy'], 'r', label='Validation acc')
axes[0, 0].set_title('Training and Validation Accuracy')
axes[0, 0].set_xlabel('Epochs')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].legend()

# Plot loss
axes[0, 1].plot(epochs, history.history['loss'], 'y', label='Training loss')
axes[0, 1].plot(epochs, history.history['val_loss'], 'r', label='Validation loss')
axes[0, 1].set_title('Training and Validation loss')
axes[0, 1].set_xlabel('Epochs')
axes[0, 1].set_ylabel('Loss')
axes[0, 1].legend()

# Plot Precision
axes[1, 0].plot(epochs, history.history['precision'], 'y', label='Training Precision')
axes[1, 0].plot(epochs, history.history['val_precision'], 'r', label='Validation Precision')
axes[1, 0].set_title('Training and Validation Precision')
axes[1, 0].set_xlabel('Epochs')
axes[1, 0].set_ylabel('Loss')
axes[1, 0].legend()

# Plot Recall
axes[1, 1].plot(epochs, history.history['recall'], 'y', label='Training Recall')
axes[1, 1].plot(epochs, history.history['val_recall'], 'r', label='Validation Recall')
axes[1, 1].set_title('Training and Validation Recall')
axes[1, 1].set_xlabel('Epochs')
axes[1, 1].set_ylabel('Loss')
axes[1, 1].legend()

# Global title
fig.suptitle(f'Training and Validation Metrics for {model.name}', fontsize=15)

# Adjust layout
plt.tight_layout()

# Save the plots
plt.savefig(results_folder + f'{model.name}_metrics.png')

plt.show()

# Get a batch of images and ground truth masks from the validation generator


# Choose a few random indices from the batch
num_samples = 5

# Plot the images, ground truth masks, and predicted masks
for i in range(num_samples):
    plt.figure(figsize=(15, 5))

    # Randomly select an index from the length of the generator
    random_index = np.random.choice(len(val_generator), replace=False)

    # Fetch a batch of data from the generator
    val_batch_x, val_batch_y = val_generator[random_index]
    # Predict on the validation batch
    predictions = model.predict(val_batch_x)
    # Original Image
    plt.subplot(1, 3, 1)
    plt.imshow(val_batch_x[0,:,:,:])
    plt.title('Original Image')
    plt.axis('off')

    # Ground Truth Mask
    plt.subplot(1, 3, 2)
    plt.imshow(val_batch_y[0, :, :, 0], cmap='gray')
    plt.title('Ground Truth Mask')
    plt.axis('off')

    # Predicted Mask
    plt.subplot(1, 3, 3)
    plt.imshow(predictions[0, :, :, 0] > 0.50, cmap='gray')
    plt.title('Predicted Mask')
    plt.axis('off')
    plt.savefig(results_folder + f'{model.name}_pred{i}.png')
    plt.show()

"""## Modified U-net 6:

- Added batch normalization layers after the convulutional layers using the original network.
"""

# Define the new model

def conv_block(x, num_filters, kernel_size=3, dropout_rate=0.5):
    x = Conv2D(num_filters, 3, padding="same", activation="relu")(x)
    x = Conv2D(num_filters, 3, padding="same", activation="relu")(x)
    x = BatchNormalization()(x) # Added for this modified network
    return x

def unet_6(input_shape=(256, 256, 1), n_classes=1):
    inputs = Input(input_shape)

    # Contracting Path
    conv1, pool1 = encoder_block(inputs, 64)
    conv2, pool2 = encoder_block(pool1, 128)
    conv3, pool3 = encoder_block(pool2, 256)
    conv4, drop4 = encoder_block(pool3, 512)

    # Bottom
    conv5 = conv_block(drop4, 1024)

    # Expansive Path
    conv6 = decoder_block(conv5, conv4, 512)
    conv7 = decoder_block(conv6, conv3, 256)
    conv8 = decoder_block(conv7, conv2, 128)
    conv9 = decoder_block(conv8, conv1, 64)

    # Output layer
    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)

    model = Model(inputs=inputs, outputs=outputs, name = 'Modified_UNet_06')

    return model

# Create the U-Net model
model = unet_6(input_shape)
model.summary()

# Train different Unet models with different learning rates
model_data = {}
lr = 0.0001

# Load optimizer
optim = tf.keras.optimizers.Adam(learning_rate = lr)
model.compile(optim, hybrid_loss, metrics=metrics)

print(f'Training {model.name}, using {lr} for the learning rate')
print()

start_train = time.time()

history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator) // 7,
    epochs=45,
    verbose=1,
    validation_data=val_generator,
    validation_steps=len(val_generator) // 10
)

model.save(weights_folder + model.name + ".hdf5")

end_train = time.time()


print()
history.history["training_time"] = end_train - start_train
print(f'Elapsed time training (learning rate {lr}) = {end_train - start_train:.4f} secs')
print()

print_best_metrics(history.history)

model_data[model.name] = history.history

# Clear the TensorFlow session and release GPU memory
tf.keras.backend.clear_session()

# Path to the JSON file
import json
json_file_path = results_folder + model.name + '_data.json'

# Save the data to a JSON file
with open(json_file_path, 'w') as json_file:
    json.dump(model_data, json_file, indent=4)

print(f'Data saved to {json_file_path}')

#plot the training and validation accuracy and loss at each epoch
# Extract accuracy and loss data
epochs = range(1, len(history.history['accuracy']) + 1)

# Create subplots with 1 row and 2 columns
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Plot accuracy
axes[0, 0].plot(epochs, history.history['accuracy'], 'y', label='Training acc')
axes[0, 0].plot(epochs, history.history['val_accuracy'], 'r', label='Validation acc')
axes[0, 0].set_title('Training and Validation Accuracy')
axes[0, 0].set_xlabel('Epochs')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].legend()

# Plot loss
axes[0, 1].plot(epochs, history.history['loss'], 'y', label='Training loss')
axes[0, 1].plot(epochs, history.history['val_loss'], 'r', label='Validation loss')
axes[0, 1].set_title('Training and Validation loss')
axes[0, 1].set_xlabel('Epochs')
axes[0, 1].set_ylabel('Loss')
axes[0, 1].legend()

# Plot Precision
axes[1, 0].plot(epochs, history.history['precision'], 'y', label='Training Precision')
axes[1, 0].plot(epochs, history.history['val_precision'], 'r', label='Validation Precision')
axes[1, 0].set_title('Training and Validation Precision')
axes[1, 0].set_xlabel('Epochs')
axes[1, 0].set_ylabel('Loss')
axes[1, 0].legend()

# Plot Recall
axes[1, 1].plot(epochs, history.history['recall'], 'y', label='Training Recall')
axes[1, 1].plot(epochs, history.history['val_recall'], 'r', label='Validation Recall')
axes[1, 1].set_title('Training and Validation Recall')
axes[1, 1].set_xlabel('Epochs')
axes[1, 1].set_ylabel('Loss')
axes[1, 1].legend()

# Global title
fig.suptitle(f'Training and Validation Metrics for {model.name}', fontsize=15)

# Adjust layout
plt.tight_layout()

# Save the plots
plt.savefig(results_folder + f'{model.name}_metrics.png')

plt.show()

# Get a batch of images and ground truth masks from the validation generator


# Choose a few random indices from the batch
num_samples = 5

# Plot the images, ground truth masks, and predicted masks
for i in range(num_samples):
    plt.figure(figsize=(15, 5))

    # Randomly select an index from the length of the generator
    random_index = np.random.choice(len(val_generator), replace=False)

    # Fetch a batch of data from the generator
    val_batch_x, val_batch_y = val_generator[random_index]
    # Predict on the validation batch
    predictions = model.predict(val_batch_x)
    # Original Image
    plt.subplot(1, 3, 1)
    plt.imshow(val_batch_x[0,:,:,:])
    plt.title('Original Image')
    plt.axis('off')

    # Ground Truth Mask
    plt.subplot(1, 3, 2)
    plt.imshow(val_batch_y[0, :, :, 0], cmap='gray')
    plt.title('Ground Truth Mask')
    plt.axis('off')

    # Predicted Mask
    plt.subplot(1, 3, 3)
    plt.imshow(predictions[0, :, :, 0] > 0.50, cmap='gray')
    plt.title('Predicted Mask')
    plt.axis('off')
    plt.savefig(results_folder + f'{model.name}_pred{i}.png')
    plt.show()

"""## Modified U-net 7:

- Added batch normalization layers after the convulutional layers using the original network.
- Reduced the number of convulutional blocks from 5 to 4.
"""

def unet_7(input_shape=(256, 256, 1)):
    inputs = Input(input_shape)

    # Contracting Path
    conv1, pool1 = encoder_block(inputs, 64)
    conv2, pool2 = encoder_block(pool1, 128)
    conv3, pool3 = encoder_block(pool2, 256)

    # Bottom
    conv4 = conv_block(pool3, 512)

    # Expansive Path
    conv5 = decoder_block(conv4, conv3, 512)
    conv6 = decoder_block(conv5, conv2, 256)
    conv7 = decoder_block(conv6, conv1, 128)

    # Output layer
    outputs = Conv2D(1, 1, activation='sigmoid')(conv7)

    model = Model(inputs=inputs, outputs=outputs, name = 'Modified_UNet_07')

    return model

# Create the U-Net model
model = unet_7(input_shape)
model.summary()

# Train different Unet models with different learning rates
model_data = {}
lr = 0.0001

# Load optimizer
optim = tf.keras.optimizers.Adam(learning_rate = lr)
model.compile(optim, hybrid_loss, metrics=metrics)

print(f'Training {model.name}, using {lr} for the learning rate')
print()

start_train = time.time()

history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator) // 7,
    epochs=45,
    verbose=1,
    validation_data=val_generator,
    validation_steps=len(val_generator) // 10
)

model.save(weights_folder + model.name + ".hdf5")

end_train = time.time()


print()
history.history["training_time"] = end_train - start_train
print(f'Elapsed time training (learning rate {lr}) = {end_train - start_train:.4f} secs')
print()

print_best_metrics(history.history)

model_data[model.name] = history.history

# Clear the TensorFlow session and release GPU memory
tf.keras.backend.clear_session()

# Path to the JSON file
import json
json_file_path = results_folder + model.name + '_data.json'

# Save the data to a JSON file
with open(json_file_path, 'w') as json_file:
    json.dump(model_data, json_file, indent=4)

print(f'Data saved to {json_file_path}')

#plot the training and validation accuracy and loss at each epoch
# Extract accuracy and loss data
epochs = range(1, len(history.history['accuracy']) + 1)

# Create subplots with 1 row and 2 columns
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Plot accuracy
axes[0, 0].plot(epochs, history.history['accuracy'], 'y', label='Training acc')
axes[0, 0].plot(epochs, history.history['val_accuracy'], 'r', label='Validation acc')
axes[0, 0].set_title('Training and Validation Accuracy')
axes[0, 0].set_xlabel('Epochs')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].legend()

# Plot loss
axes[0, 1].plot(epochs, history.history['loss'], 'y', label='Training loss')
axes[0, 1].plot(epochs, history.history['val_loss'], 'r', label='Validation loss')
axes[0, 1].set_title('Training and Validation loss')
axes[0, 1].set_xlabel('Epochs')
axes[0, 1].set_ylabel('Loss')
axes[0, 1].legend()

# Plot Precision
axes[1, 0].plot(epochs, history.history['precision'], 'y', label='Training Precision')
axes[1, 0].plot(epochs, history.history['val_precision'], 'r', label='Validation Precision')
axes[1, 0].set_title('Training and Validation Precision')
axes[1, 0].set_xlabel('Epochs')
axes[1, 0].set_ylabel('Loss')
axes[1, 0].legend()

# Plot Recall
axes[1, 1].plot(epochs, history.history['recall'], 'y', label='Training Recall')
axes[1, 1].plot(epochs, history.history['val_recall'], 'r', label='Validation Recall')
axes[1, 1].set_title('Training and Validation Recall')
axes[1, 1].set_xlabel('Epochs')
axes[1, 1].set_ylabel('Loss')
axes[1, 1].legend()

# Global title
fig.suptitle(f'Training and Validation Metrics for {model.name}', fontsize=15)

# Adjust layout
plt.tight_layout()

# Save the plots
plt.savefig(results_folder + f'{model.name}_metrics.png')

plt.show()

# Get a batch of images and ground truth masks from the validation generator


# Choose a few random indices from the batch
num_samples = 5

# Plot the images, ground truth masks, and predicted masks
for i in range(num_samples):
    plt.figure(figsize=(15, 5))

    # Randomly select an index from the length of the generator
    random_index = np.random.choice(len(val_generator), replace=False)

    # Fetch a batch of data from the generator
    val_batch_x, val_batch_y = val_generator[random_index]
    # Predict on the validation batch
    predictions = model.predict(val_batch_x)
    # Original Image
    plt.subplot(1, 3, 1)
    plt.imshow(val_batch_x[0,:,:,:])
    plt.title('Original Image')
    plt.axis('off')

    # Ground Truth Mask
    plt.subplot(1, 3, 2)
    plt.imshow(val_batch_y[0, :, :, 0], cmap='gray')
    plt.title('Ground Truth Mask')
    plt.axis('off')

    # Predicted Mask
    plt.subplot(1, 3, 3)
    plt.imshow(predictions[0, :, :, 0] > 0.50, cmap='gray')
    plt.title('Predicted Mask')
    plt.axis('off')
    plt.savefig(results_folder + f'{model.name}_pred{i}.png')
    plt.show()

"""## Modified U-net 8:

- Added batch normalization layers between the convulutional layers and activation layers using the original network.
"""

def conv_block(input, num_filters):
    x = Conv2D(num_filters, 3, padding="same")(input)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)

    x = Conv2D(num_filters, 3, padding="same")(x)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)

    return x

def unet_8(input_shape=(256, 256, 1), n_classes=1):
    inputs = Input(input_shape)

    # Contracting Path
    conv1, pool1 = encoder_block(inputs, 64)
    conv2, pool2 = encoder_block(pool1, 128)
    conv3, pool3 = encoder_block(pool2, 256)
    conv4, drop4 = encoder_block(pool3, 512)

    # Bottom
    conv5 = conv_block(drop4, 1024)

    # Expansive Path
    conv6 = decoder_block(conv5, conv4, 512)
    conv7 = decoder_block(conv6, conv3, 256)
    conv8 = decoder_block(conv7, conv2, 128)
    conv9 = decoder_block(conv8, conv1, 64)

    # Output layer
    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)

    model = Model(inputs=inputs, outputs=outputs, name = 'Modified_UNet_08')

    return model

# Create the U-Net model
model = unet_8(input_shape)
model.summary()

# Train different Unet models with different learning rates
model_data = {}
lr = 0.0001

# Load optimizer
optim = tf.keras.optimizers.Adam(learning_rate = lr)
model.compile(optim, hybrid_loss, metrics=metrics)

print(f'Training {model.name}, using {lr} for the learning rate')
print()

start_train = time.time()

history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator) // 7,
    epochs=45,
    verbose=1,
    validation_data=val_generator,
    validation_steps=len(val_generator) // 10
)

model.save(weights_folder + model.name + ".hdf5")

end_train = time.time()


print()
history.history["training_time"] = end_train - start_train
print(f'Elapsed time training (learning rate {lr}) = {end_train - start_train:.4f} secs')
print()

print_best_metrics(history.history)

model_data[model.name] = history.history

# Clear the TensorFlow session and release GPU memory
tf.keras.backend.clear_session()

# Path to the JSON file
import json
json_file_path = results_folder + model.name + '_data.json'

# Save the data to a JSON file
with open(json_file_path, 'w') as json_file:
    json.dump(model_data, json_file, indent=4)

print(f'Data saved to {json_file_path}')

#plot the training and validation accuracy and loss at each epoch
# Extract accuracy and loss data
epochs = range(1, len(history.history['accuracy']) + 1)

# Create subplots with 1 row and 2 columns
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Plot accuracy
axes[0, 0].plot(epochs, history.history['accuracy'], 'y', label='Training acc')
axes[0, 0].plot(epochs, history.history['val_accuracy'], 'r', label='Validation acc')
axes[0, 0].set_title('Training and Validation Accuracy')
axes[0, 0].set_xlabel('Epochs')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].legend()

# Plot loss
axes[0, 1].plot(epochs, history.history['loss'], 'y', label='Training loss')
axes[0, 1].plot(epochs, history.history['val_loss'], 'r', label='Validation loss')
axes[0, 1].set_title('Training and Validation loss')
axes[0, 1].set_xlabel('Epochs')
axes[0, 1].set_ylabel('Loss')
axes[0, 1].legend()

# Plot Precision
axes[1, 0].plot(epochs, history.history['precision'], 'y', label='Training Precision')
axes[1, 0].plot(epochs, history.history['val_precision'], 'r', label='Validation Precision')
axes[1, 0].set_title('Training and Validation Precision')
axes[1, 0].set_xlabel('Epochs')
axes[1, 0].set_ylabel('Loss')
axes[1, 0].legend()

# Plot Recall
axes[1, 1].plot(epochs, history.history['recall'], 'y', label='Training Recall')
axes[1, 1].plot(epochs, history.history['val_recall'], 'r', label='Validation Recall')
axes[1, 1].set_title('Training and Validation Recall')
axes[1, 1].set_xlabel('Epochs')
axes[1, 1].set_ylabel('Loss')
axes[1, 1].legend()

# Global title
fig.suptitle(f'Training and Validation Metrics for {model.name}', fontsize=15)

# Adjust layout
plt.tight_layout()

# Save the plots
plt.savefig(results_folder + f'{model.name}_metrics.png')

plt.show()

# Get a batch of images and ground truth masks from the validation generator


# Choose a few random indices from the batch
num_samples = 5

# Plot the images, ground truth masks, and predicted masks
for i in range(num_samples):
    plt.figure(figsize=(15, 5))

    # Randomly select an index from the length of the generator
    random_index = np.random.choice(len(val_generator), replace=False)

    # Fetch a batch of data from the generator
    val_batch_x, val_batch_y = val_generator[random_index]
    # Predict on the validation batch
    predictions = model.predict(val_batch_x)
    # Original Image
    plt.subplot(1, 3, 1)
    plt.imshow(val_batch_x[0,:,:,:])
    plt.title('Original Image')
    plt.axis('off')

    # Ground Truth Mask
    plt.subplot(1, 3, 2)
    plt.imshow(val_batch_y[0, :, :, 0], cmap='gray')
    plt.title('Ground Truth Mask')
    plt.axis('off')

    # Predicted Mask
    plt.subplot(1, 3, 3)
    plt.imshow(predictions[0, :, :, 0] > 0.50, cmap='gray')
    plt.title('Predicted Mask')
    plt.axis('off')
    plt.savefig(results_folder + f'{model.name}_pred{i}.png')
    plt.show()

"""## Modified U-net 9:

- Added batch normalization layers between the convulutional layers and activation layers using the original network.
- Reduced the number of convulutional blocks from 5 to 4.
"""

def unet_9(input_shape=(256, 256, 1)):
    inputs = Input(input_shape)

    # Contracting Path
    conv1, pool1 = encoder_block(inputs, 64)
    conv2, pool2 = encoder_block(pool1, 128)
    conv3, pool3 = encoder_block(pool2, 256)

    # Bottom
    conv4 = conv_block(pool3, 512)

    # Expansive Path
    conv5 = decoder_block(conv4, conv3, 512)
    conv6 = decoder_block(conv5, conv2, 256)
    conv7 = decoder_block(conv6, conv1, 128)

    # Output layer
    outputs = Conv2D(1, 1, activation='sigmoid')(conv7)

    model = Model(inputs=inputs, outputs=outputs, name = 'Modified_UNet_09')

    return model

# Create the U-Net model
model = unet_9(input_shape)
model.summary()

# Train different Unet models with different learning rates
model_data = {}
lr = 0.0001

# Load optimizer
optim = tf.keras.optimizers.Adam(learning_rate = lr)
model.compile(optim, hybrid_loss, metrics=metrics)

print(f'Training {model.name}, using {lr} for the learning rate')
print()

start_train = time.time()

history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator) // 7,
    epochs=45,
    verbose=1,
    validation_data=val_generator,
    validation_steps=len(val_generator) // 10
)

model.save(weights_folder + model.name + ".hdf5")

end_train = time.time()


print()
history.history["training_time"] = end_train - start_train
print(f'Elapsed time training (learning rate {lr}) = {end_train - start_train:.4f} secs')
print()

print_best_metrics(history.history)

model_data[model.name] = history.history

# Clear the TensorFlow session and release GPU memory
tf.keras.backend.clear_session()

# Path to the JSON file
import json
json_file_path = results_folder + model.name + '_data.json'

# Save the data to a JSON file
with open(json_file_path, 'w') as json_file:
    json.dump(model_data, json_file, indent=4)

print(f'Data saved to {json_file_path}')

#plot the training and validation accuracy and loss at each epoch
# Extract accuracy and loss data
epochs = range(1, len(history.history['accuracy']) + 1)

# Create subplots with 1 row and 2 columns
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Plot accuracy
axes[0, 0].plot(epochs, history.history['accuracy'], 'y', label='Training acc')
axes[0, 0].plot(epochs, history.history['val_accuracy'], 'r', label='Validation acc')
axes[0, 0].set_title('Training and Validation Accuracy')
axes[0, 0].set_xlabel('Epochs')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].legend()

# Plot loss
axes[0, 1].plot(epochs, history.history['loss'], 'y', label='Training loss')
axes[0, 1].plot(epochs, history.history['val_loss'], 'r', label='Validation loss')
axes[0, 1].set_title('Training and Validation loss')
axes[0, 1].set_xlabel('Epochs')
axes[0, 1].set_ylabel('Loss')
axes[0, 1].legend()

# Plot Precision
axes[1, 0].plot(epochs, history.history['precision'], 'y', label='Training Precision')
axes[1, 0].plot(epochs, history.history['val_precision'], 'r', label='Validation Precision')
axes[1, 0].set_title('Training and Validation Precision')
axes[1, 0].set_xlabel('Epochs')
axes[1, 0].set_ylabel('Loss')
axes[1, 0].legend()

# Plot Recall
axes[1, 1].plot(epochs, history.history['recall'], 'y', label='Training Recall')
axes[1, 1].plot(epochs, history.history['val_recall'], 'r', label='Validation Recall')
axes[1, 1].set_title('Training and Validation Recall')
axes[1, 1].set_xlabel('Epochs')
axes[1, 1].set_ylabel('Loss')
axes[1, 1].legend()

# Global title
fig.suptitle(f'Training and Validation Metrics for {model.name}', fontsize=15)

# Adjust layout
plt.tight_layout()

# Save the plots
plt.savefig(results_folder + f'{model.name}_metrics.png')

plt.show()

# Get a batch of images and ground truth masks from the validation generator


# Choose a few random indices from the batch
num_samples = 5

# Plot the images, ground truth masks, and predicted masks
for i in range(num_samples):
    plt.figure(figsize=(15, 5))

    # Randomly select an index from the length of the generator
    random_index = np.random.choice(len(val_generator), replace=False)

    # Fetch a batch of data from the generator
    val_batch_x, val_batch_y = val_generator[random_index]
    # Predict on the validation batch
    predictions = model.predict(val_batch_x)
    # Original Image
    plt.subplot(1, 3, 1)
    plt.imshow(val_batch_x[0,:,:,:])
    plt.title('Original Image')
    plt.axis('off')

    # Ground Truth Mask
    plt.subplot(1, 3, 2)
    plt.imshow(val_batch_y[0, :, :, 0], cmap='gray')
    plt.title('Ground Truth Mask')
    plt.axis('off')

    # Predicted Mask
    plt.subplot(1, 3, 3)
    plt.imshow(predictions[0, :, :, 0] > 0.50, cmap='gray')
    plt.title('Predicted Mask')
    plt.axis('off')
    plt.savefig(results_folder + f'{model.name}_pred{i}.png')
    plt.show()

"""## Modified U-net 10:

- Added batch normalization layers between the convulutional layers and activation layers using the original network.
- Added dropout layer after the bacth normalization layer

"""

def conv_block(input, num_filters, dropout_rate = 0.5):
    x = Conv2D(num_filters, 3, padding="same")(input)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)
    x = Dropout(dropout_rate)(x)

    x = Conv2D(num_filters, 3, padding="same")(x)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)
    x = Dropout(dropout_rate)(x)

    return x

def unet_10(input_shape=(256, 256, 1), n_classes=1):
    inputs = Input(input_shape)

    # Contracting Path
    conv1, pool1 = encoder_block(inputs, 64)
    conv2, pool2 = encoder_block(pool1, 128)
    conv3, pool3 = encoder_block(pool2, 256)
    conv4, drop4 = encoder_block(pool3, 512)

    # Bottom
    conv5 = conv_block(drop4, 1024)

    # Expansive Path
    conv6 = decoder_block(conv5, conv4, 512)
    conv7 = decoder_block(conv6, conv3, 256)
    conv8 = decoder_block(conv7, conv2, 128)
    conv9 = decoder_block(conv8, conv1, 64)

    # Output layer
    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)

    model = Model(inputs=inputs, outputs=outputs, name = 'Modified_UNet_10')

    return model

# Create the U-Net model
model = unet_10(input_shape)
model.summary()

# Train different Unet models with different learning rates
model_data = {}
lr = 0.0001

# Load optimizer
optim = tf.keras.optimizers.Adam(learning_rate = lr)
model.compile(optim, hybrid_loss, metrics=metrics)

print(f'Training {model.name}, using {lr} for the learning rate')
print()

start_train = time.time()

history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator) // 7,
    epochs=45,
    verbose=1,
    validation_data=val_generator,
    validation_steps=len(val_generator) // 10
)

model.save(weights_folder + model.name + ".hdf5")

end_train = time.time()


print()
history.history["training_time"] = end_train - start_train
print(f'Elapsed time training (learning rate {lr}) = {end_train - start_train:.4f} secs')
print()

print_best_metrics(history.history)

model_data[model.name] = history.history

# Clear the TensorFlow session and release GPU memory
tf.keras.backend.clear_session()

# Path to the JSON file
import json
json_file_path = results_folder + model.name + '_data.json'

# Save the data to a JSON file
with open(json_file_path, 'w') as json_file:
    json.dump(model_data, json_file, indent=4)

print(f'Data saved to {json_file_path}')

#plot the training and validation accuracy and loss at each epoch
# Extract accuracy and loss data
epochs = range(1, len(history.history['accuracy']) + 1)

# Create subplots with 1 row and 2 columns
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Plot accuracy
axes[0, 0].plot(epochs, history.history['accuracy'], 'y', label='Training acc')
axes[0, 0].plot(epochs, history.history['val_accuracy'], 'r', label='Validation acc')
axes[0, 0].set_title('Training and Validation Accuracy')
axes[0, 0].set_xlabel('Epochs')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].legend()

# Plot loss
axes[0, 1].plot(epochs, history.history['loss'], 'y', label='Training loss')
axes[0, 1].plot(epochs, history.history['val_loss'], 'r', label='Validation loss')
axes[0, 1].set_title('Training and Validation loss')
axes[0, 1].set_xlabel('Epochs')
axes[0, 1].set_ylabel('Loss')
axes[0, 1].legend()

# Plot Precision
axes[1, 0].plot(epochs, history.history['precision'], 'y', label='Training Precision')
axes[1, 0].plot(epochs, history.history['val_precision'], 'r', label='Validation Precision')
axes[1, 0].set_title('Training and Validation Precision')
axes[1, 0].set_xlabel('Epochs')
axes[1, 0].set_ylabel('Loss')
axes[1, 0].legend()

# Plot Recall
axes[1, 1].plot(epochs, history.history['recall'], 'y', label='Training Recall')
axes[1, 1].plot(epochs, history.history['val_recall'], 'r', label='Validation Recall')
axes[1, 1].set_title('Training and Validation Recall')
axes[1, 1].set_xlabel('Epochs')
axes[1, 1].set_ylabel('Loss')
axes[1, 1].legend()

# Global title
fig.suptitle(f'Training and Validation Metrics for {model.name}', fontsize=15)

# Adjust layout
plt.tight_layout()

# Save the plots
plt.savefig(results_folder + f'{model.name}_metrics.png')

plt.show()

# Get a batch of images and ground truth masks from the validation generator


# Choose a few random indices from the batch
num_samples = 5

# Plot the images, ground truth masks, and predicted masks
for i in range(num_samples):
    plt.figure(figsize=(15, 5))

    # Randomly select an index from the length of the generator
    random_index = np.random.choice(len(val_generator), replace=False)

    # Fetch a batch of data from the generator
    val_batch_x, val_batch_y = val_generator[random_index]
    # Predict on the validation batch
    predictions = model.predict(val_batch_x)
    # Original Image
    plt.subplot(1, 3, 1)
    plt.imshow(val_batch_x[0,:,:,:])
    plt.title('Original Image')
    plt.axis('off')

    # Ground Truth Mask
    plt.subplot(1, 3, 2)
    plt.imshow(val_batch_y[0, :, :, 0], cmap='gray')
    plt.title('Ground Truth Mask')
    plt.axis('off')

    # Predicted Mask
    plt.subplot(1, 3, 3)
    plt.imshow(predictions[0, :, :, 0] > 0.50, cmap='gray')
    plt.title('Predicted Mask')
    plt.axis('off')
    plt.savefig(results_folder + f'{model.name}_pred{i}.png')
    plt.show()

"""## Modified U-net 11:

- Added batch normalization layers between the convulutional layers and activation layers using the original network.
- Added dropout layer after the batch normalization layer
- Reduced the number of convolutional blocks from 5 to 4.
"""

def unet_11(input_shape=(256, 256, 1)):
    inputs = Input(input_shape)

    # Contracting Path
    conv1, pool1 = encoder_block(inputs, 64)
    conv2, pool2 = encoder_block(pool1, 128)
    conv3, pool3 = encoder_block(pool2, 256)

    # Bottom
    conv4 = conv_block(pool3, 512)

    # Expansive Path
    conv5 = decoder_block(conv4, conv3, 512)
    conv6 = decoder_block(conv5, conv2, 256)
    conv7 = decoder_block(conv6, conv1, 128)

    # Output layer
    outputs = Conv2D(1, 1, activation='sigmoid')(conv7)

    model = Model(inputs=inputs, outputs=outputs, name = 'Modified_UNet_11')

    return model

# Create the U-Net model
model = unet_11(input_shape)
model.summary()

# Train different Unet models with different learning rates
model_data = {}
lr = 0.0001

# Load optimizer
optim = tf.keras.optimizers.Adam(learning_rate = lr)
model.compile(optim, hybrid_loss, metrics=metrics)

print(f'Training {model.name}, using {lr} for the learning rate')
print()

start_train = time.time()

history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator) // 7,
    epochs=45,
    verbose=1,
    validation_data=val_generator,
    validation_steps=len(val_generator) // 10
)

model.save(weights_folder + model.name + ".hdf5")

end_train = time.time()


print()
history.history["training_time"] = end_train - start_train
print(f'Elapsed time training (learning rate {lr}) = {end_train - start_train:.4f} secs')
print()

print_best_metrics(history.history)

model_data[model.name] = history.history

# Clear the TensorFlow session and release GPU memory
tf.keras.backend.clear_session()

# Path to the JSON file
import json
json_file_path = results_folder + model.name + '_data.json'

# Save the data to a JSON file
with open(json_file_path, 'w') as json_file:
    json.dump(model_data, json_file, indent=4)

print(f'Data saved to {json_file_path}')

#plot the training and validation accuracy and loss at each epoch
# Extract accuracy and loss data
epochs = range(1, len(history.history['accuracy']) + 1)

# Create subplots with 1 row and 2 columns
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Plot accuracy
axes[0, 0].plot(epochs, history.history['accuracy'], 'y', label='Training acc')
axes[0, 0].plot(epochs, history.history['val_accuracy'], 'r', label='Validation acc')
axes[0, 0].set_title('Training and Validation Accuracy')
axes[0, 0].set_xlabel('Epochs')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].legend()

# Plot loss
axes[0, 1].plot(epochs, history.history['loss'], 'y', label='Training loss')
axes[0, 1].plot(epochs, history.history['val_loss'], 'r', label='Validation loss')
axes[0, 1].set_title('Training and Validation loss')
axes[0, 1].set_xlabel('Epochs')
axes[0, 1].set_ylabel('Loss')
axes[0, 1].legend()

# Plot Precision
axes[1, 0].plot(epochs, history.history['precision'], 'y', label='Training Precision')
axes[1, 0].plot(epochs, history.history['val_precision'], 'r', label='Validation Precision')
axes[1, 0].set_title('Training and Validation Precision')
axes[1, 0].set_xlabel('Epochs')
axes[1, 0].set_ylabel('Loss')
axes[1, 0].legend()

# Plot Recall
axes[1, 1].plot(epochs, history.history['recall'], 'y', label='Training Recall')
axes[1, 1].plot(epochs, history.history['val_recall'], 'r', label='Validation Recall')
axes[1, 1].set_title('Training and Validation Recall')
axes[1, 1].set_xlabel('Epochs')
axes[1, 1].set_ylabel('Loss')
axes[1, 1].legend()

# Global title
fig.suptitle(f'Training and Validation Metrics for {model.name}', fontsize=15)

# Adjust layout
plt.tight_layout()

# Save the plots
plt.savefig(results_folder + f'{model.name}_metrics.png')

plt.show()

# Get a batch of images and ground truth masks from the validation generator


# Choose a few random indices from the batch
num_samples = 5

# Plot the images, ground truth masks, and predicted masks
for i in range(num_samples):
    plt.figure(figsize=(15, 5))

    # Randomly select an index from the length of the generator
    random_index = np.random.choice(len(val_generator), replace=False)

    # Fetch a batch of data from the generator
    val_batch_x, val_batch_y = val_generator[random_index]
    # Predict on the validation batch
    predictions = model.predict(val_batch_x)
    # Original Image
    plt.subplot(1, 3, 1)
    plt.imshow(val_batch_x[0,:,:,:])
    plt.title('Original Image')
    plt.axis('off')

    # Ground Truth Mask
    plt.subplot(1, 3, 2)
    plt.imshow(val_batch_y[0, :, :, 0], cmap='gray')
    plt.title('Ground Truth Mask')
    plt.axis('off')

    # Predicted Mask
    plt.subplot(1, 3, 3)
    plt.imshow(predictions[0, :, :, 0] > 0.50, cmap='gray')
    plt.title('Predicted Mask')
    plt.axis('off')
    plt.savefig(results_folder + f'{model.name}_pred{i}.png')
    plt.show()